{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 5571\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0     0                      Ok lar... Joking wif u oni...\n",
       "1     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2     0  U dun say so early hor... U c already then say...\n",
       "3     0  Nah I don't think he goes to usf, he lives aro...\n",
       "4     1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "5     0  Even my brother is not like to speak with me. ...\n",
       "6     0  As per your request 'Melle Melle (Oru Minnamin...\n",
       "7     1  WINNER!! As a valued network customer you have...\n",
       "8     1  Had your mobile 11 months or more? U R entitle...\n",
       "9     0  I'm gonna be home soon and i don't want to tal..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./dataset/SMSSpamCollection', sep=\"\\t\")\n",
    "dataset.columns = ['spam', 'text']\n",
    "dataset.spam = dataset.spam.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(f\"Size of the dataset: {dataset.shape[0]}\")\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning:\n",
    "    Clear and get only the main part from the dataset\n",
    "    Ex: remove the tags of the html.\n",
    "    Ex: filter the texts in PDF and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Remove the HTML tags\n",
    "    text = re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    text = re.sub(\"<.*?>\",\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalization:\n",
    "    Remove the pontuation, tags, put everything in same case and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \",text)\n",
    "    \n",
    "    text = text.replace('  ', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization:\n",
    "    Split the text in words spliting by the whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):    \n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop Words:\n",
    "    They are words witch don't get no one meaning, they are just used to complement the context,\n",
    "    and to connect the terms.\n",
    "    Ex: 'i', 'you', 'in', 'out', 'are', 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words, they are words witch don't give no one especific meaning\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming:\n",
    "    Takes of the variation of the words and remove the finally to combine than.\n",
    "    Ex: 'change', 'changing', 'changes' => 'chang'\n",
    "\n",
    "\n",
    "### 6. Lemmatization:\n",
    "    Takes the variation of the same word and convert to the same one (Noun).\n",
    "    Ex: 'is', 'were', 'was' => 'be'\n",
    "    Ex: 'ones' => 'one'\n",
    "\n",
    "    Part of Speech(PoS) (Verb):\n",
    "    Ex: 'bored' => 'bore'\n",
    "    Ex: 'stating' => 'start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Remove the pural\n",
    "# Remove the verb conjugation\n",
    "def stem(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "# Remove the personality\n",
    "def lem(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tag filtering\n",
    "    Filter the words according with the sintaxe definition like a noun, verbs, adverbs e etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# RB | RB | JJ | NN | NNP | JJ | JJS | IN | VB | VBZ | VBD | VBG\n",
    "\n",
    "# IN = preposition/subordinating conjunction\n",
    "\n",
    "# RB = adverb very, silently\n",
    "# RBR = adverb, comparative better\n",
    "# RBS = adverb, superlative best\n",
    "# RP = particle give up\n",
    "\n",
    "# IN = preposition/subordinating conjunction\n",
    "# JJ = adjective ‘big’\n",
    "# JJR = adjective, comparative ‘bigger’\n",
    "# JJS = adjective, superlative ‘biggest’\n",
    "\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "\n",
    "# Filter using regular array\n",
    "def filter_tokens(tokens):\n",
    "    tags = [x[1] for x in nltk.pos_tag(list(tokens))]\n",
    "    filters = (\"RB\", \"RBR\", \"RBS\", \"RP\", \"JJ\", \"JJR\", \"JJS\", \"JJ\", \"VB\")\n",
    "    \n",
    "    return [tokens[i] for i in range(len(tokens)) if tags[i] in filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove the numbers\n",
    "    In this case it's remove the numbers inside because can variaty accourding with the qty, because inside the dataset has \n",
    "    numbers, specially the spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numbers(tokens):\n",
    "    return [\"number\" if bool(re.search(r'\\d', w)) else w for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We wee create a our vocabullary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Normalized!\n",
      "> Tokenized!\n",
      "> Removed the stop words!\n",
      "> Merged the term to the root form!\n",
      "> Transformed the numbers\n",
      "\n",
      "Colection [:100]:\n",
      "\n",
      "['ok', 'lar', 'joking', 'wif', 'u', 'oni', 'free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply', 'u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say', 'nah', 'think', 'go', 'usf', 'life', 'around', 'though', 'freemsg', 'hey', 'darling', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'rcv', 'even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent', 'per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'caller', 'press', 'copy', 'friend', 'callertune', 'winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hour', 'mobile', 'month', 'u']\n"
     ]
    }
   ],
   "source": [
    "all_words = \" \".join(list(dataset.text))\n",
    "\n",
    "text = normalize(clean(all_words))\n",
    "print(\"> Normalized!\")\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "print(\"> Tokenized!\")\n",
    "\n",
    "tokens = remove_stopwords(tokens)\n",
    "print(\"> Removed the stop words!\")\n",
    "\n",
    "# tokens = stem(tokens)\n",
    "tokens = lem(tokens)\n",
    "print(\"> Merged the term to the root form!\")\n",
    "\n",
    "# tokens = filter_tokens(tokens)\n",
    "# print(\"> Filtred by tags witch get more meaning!\")\n",
    "\n",
    "tokens = transform_numbers(tokens)\n",
    "print(\"> Transformed the numbers\")\n",
    "\n",
    "print(\"\\nColection [:100]:\\n\")\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequency of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back of words size: 7125\n",
      "7125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>call</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ur</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gt</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words  frequency\n",
       "0     u       1295\n",
       "1  call        641\n",
       "2   get        408\n",
       "3    ur        391\n",
       "4    gt        318"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "# Create the bag of words dataframe\n",
    "bag_of_words = pd.DataFrame({\"words\": list(frequency.keys()), \"frequency\": list(frequency.values())})\n",
    "\n",
    "# Order by the Frequency\n",
    "bag_of_words.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "bag_of_words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the bag of words\n",
    "bag_of_words.to_csv('./dataset/bag-of-words.csv', index=True)\n",
    "\n",
    "print(f\"Back of words size: {bag_of_words.shape[0]}\")\n",
    "\n",
    "print(bag_of_words.shape[0])\n",
    "bag_of_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the frequency in Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_single_str(words, frequency):\n",
    "    words = list(words)\n",
    "    frequency = list(frequency)\n",
    "    \n",
    "    return \" \".join([(words[i] + \" \") * frequency[i] for i in range(len(frequency))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1300x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_cloud = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(to_single_str(bag_of_words[\"words\"], bag_of_words[\"frequency\"]))\n",
    "\n",
    "plt.figure(figsize=(13, 13))\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()\n",
    "\n",
    "del word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_words\n",
    "del text\n",
    "del tokens\n",
    "del frequency\n",
    "del bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictinary\n",
    "bag_of_words = pd.read_csv('./dataset/bag-of-words.csv')\n",
    "bag_of_words_array = bag_of_words.words.values\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('./dataset/SMSSpamCollection', sep=\"\\t\")\n",
    "dataset.columns = ['spam', 'text']\n",
    "dataset.spam = dataset.spam.map({'ham': 0, 'spam': 1})\n",
    "inputs = dataset.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a tf-idf function\n",
    "def tf_idf(txt, vocabulary=None):\n",
    "    txt = list(txt)\n",
    "\n",
    "    tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word', vocabulary=vocabulary)\n",
    "    txt_transformed = tf.fit(txt).transform(txt)\n",
    "\n",
    "    return txt_transformed.toarray(), tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>call</th>\n",
       "      <th>get</th>\n",
       "      <th>ur</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>go</th>\n",
       "      <th>day</th>\n",
       "      <th>ok</th>\n",
       "      <th>free</th>\n",
       "      <th>...</th>\n",
       "      <th>cast</th>\n",
       "      <th>thkin</th>\n",
       "      <th>resubbing</th>\n",
       "      <th>shadow</th>\n",
       "      <th>breadstick</th>\n",
       "      <th>saeed</th>\n",
       "      <th>purple</th>\n",
       "      <th>yelow</th>\n",
       "      <th>brown</th>\n",
       "      <th>bitching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.191608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.315412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.315412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.383216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 7125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     u      call  get   ur   gt   lt   go  day       ok      free  ...  cast  \\\n",
       "0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  3.99054  0.000000  ...   0.0   \n",
       "1  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  4.191608  ...   0.0   \n",
       "2  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  ...   0.0   \n",
       "3  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  ...   0.0   \n",
       "4  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  3.99054  0.000000  ...   0.0   \n",
       "5  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  ...   0.0   \n",
       "6  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  ...   0.0   \n",
       "7  0.0  3.315412  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  ...   0.0   \n",
       "8  0.0  3.315412  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  8.383216  ...   0.0   \n",
       "9  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  ...   0.0   \n",
       "\n",
       "   thkin  resubbing  shadow  breadstick  saeed  purple  yelow  brown  bitching  \n",
       "0    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "1    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "2    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "3    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "4    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "5    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "6    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "7    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "8    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "9    0.0        0.0     0.0         0.0    0.0     0.0    0.0    0.0       0.0  \n",
       "\n",
       "[10 rows x 7125 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf, feature_names = tf_idf(inputs, bag_of_words_array)\n",
    "\n",
    "view = pd.DataFrame(tfidf, columns=feature_names)\n",
    "view.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tfidf\n",
    "y = dataset.spam.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a Multinomial bayes model from sklearn\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9366028708133971\n"
     ]
    }
   ],
   "source": [
    "# Predict Class\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
